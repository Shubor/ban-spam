\documentclass[10pt, a4paper]{article}

\usepackage{array}
\usepackage{float}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{amssymb,amsmath}
\usepackage[showframe=false]{geometry}
\usepackage[font=small,format=plain,labelfont={bf,up}]{caption}

\usepackage[T3,T1]{fontenc}
\DeclareSymbolFont{tipa}{T3}{cmr}{m}{n}
\DeclareMathAccent{\invbreve}{\mathalpha}{tipa}{16}

\usepackage{fancyhdr}
\fancyfoot[C]{\thepage}
\lhead{COMP3608}
\rhead{Email Classification}
\pagestyle{fancy}

\title{Email Classification with Na\"ive Bayes}
\author{Mitchell Della Marta, Shu Bor}
\date{}

% \captionsetup{format = upper}

\begin{document}

\renewcommand\thetable{\Roman{table}}

\maketitle

\section{Introduction}

\subsection{Aim}

% State aim of study, and write a paragraph about why the problem is important
To implement text classification of emails using the Na\"ive Bayes classifier for spam detection.

\subsection{Importance}

As email is one of the main forms of communication, spam detection to remove spam is important. Email spam is an unsolved global issue that negatively affects productivity and uses up valuable resources. Spam emails are usually sent with malicious intent to users for some monetary gain. Email users spend a large amount of time regularly deleting these spam emails, which needlessly occupy storage space and consumes bandwidth. Hence the development of classifiers that are able to separate legitimate from spam emails is required. %Classification of emails to separate and remove spam from other emails, aims to save the user from having to spend time and effort sorting through and deleting spam, and protect unaware users from malware.

Traditional non-machine learning based techniques are easily fooled by spammers and require periodic manual updates. Therefore, machine learning based methods are required, which are able to automatically analyse email contents and dynamically update themselves to cope with new spamming techniques.

\section{Data Preprocessing}

% Briefly describe what you did.

Before classification can be performed, we must represent the files in our sample set \emph{Lingpspam-mini600} appropriately. Using the ``bag-of-words'' model, words are extracted from the file and treated as features. There are two main characteristics in an email; the subject and the body. Thus, we will construct two ``bags-of-words'', one for each component. To determine which corpus a feature will belong to, we if the line begins with ``Subject:''. If it does the words within the line will be added to the subject corpus, otherwise they will be added to the body corpus.

To each file, we performed the following steps:

\begin{enumerate}
	\item Replace all punctuation and special symbols with a space.
	\item Remove stop words, using a list of stop words ``english.stop''.
	\item Remove numbers or words containing digits.
\end{enumerate}

The tokenisation schemes used in the example data are different to that used in our stop words list. The data detaches clitics and contraction affices, whereas the stop words list is tokenised based on whitespace and punctuation characters. Therefore, we chose to replace punctuation with a space (e.g. ``n't'' becomes ``n'', ``t'') instead of simply removing them. This meant that the stop word list would remove clitics as it removes single characters, instead of keeping it as a feature as (``nt'' isn't in the stop words list).

The \emph{document frequency} feature selection method was then used to select the 200 words which occured in the most documents, and used to build a classifier for the given documents. The following steps were taken to accomplish this:

\begin{enumerate}
\item Counter is created for each email, counting the number of times each word appears in that email.
\item For each word that appeared in the counter for an email, it is added once to the counter for the corresponding subcorpus.
\item In each corpus the words with the top 200 document frequency score, are selected as features to represent that corpus.
\end{enumerate}

Each selected feature was then weighted using its \emph{td-idf} score\cite{sebastiani}.

$$ tfidf(t_k, d_j) = \#(t_k, d_j) \times \log \frac{|Tr|}{\#Tr(t_k)} $$

Then the scores were normalised, using \emph{cosine normalisation}\cite{sebastiani}.

$$ w_{kj} = \frac{tfidf(t_k, d_j)}{\sqrt{\sum_{s=1}^{|T|} (tfidf(t_s, d_j))^2 }} $$

% The \emph{document frequency} feature selection method was used, to select the 200 words which occured in the most documents as features, and used to build a classifier for the given documents. The features are then weighted using \emph{td-idf} and normalised using \emph{cosine normalisation}\cite{sebastiani}.

% \begin{enumerate}
% \item For each file in the sample set \emph{Lingpspam-mini600}:
% 	\begin{enumerate}[label*=\arabic*.]
% 	\item Open the file and read it line by line.
% 	\item Replace all punctuation and special symbols from each line with a space (e.g. ``don't" becomes ``don", ``t").
% 	\item The sub-corpus the line belongs to is determined by checking if the line begins with ``Subject:". If it does the words within the line will be added to the subject corpus, otherwise they will be added to the body corpus.
% 	\item Words from each line are extracted using space as a delimiter.
% 	\item Stop words, numbers and words containing digits are removed.
% 	\item A counter is created for both corpora. For each word that appears in the email, it is added to its respective counter.
% 	\item For each word that appeared in the counter for an email, it is added once to the counter for the corresponding subcorpus.
% 	\end{enumerate}
% \item In each corpus the words with the top 200 document frequency score (words which appear in the most number of documents) are selected as features for that corpus.
% \item For each feature, its \emph{tf-idf} score is calculated.
% $$ tfidf(t_k, d_j) = \#(t_k, d_j) \times \log \frac{|Tr|}{\#Tr(t_k)} $$
% \item The \emph{tf-idf} values are normalised using \emph{cosine normalisation}.
% $$ w_{kj} = \frac{tfidf(t_k, d_j)}{\sqrt{\sum_{s=1}^{|T|} (tfidf(t_s, d_j))^2 }} $$
% \end{enumerate}

\subsection{Data Characteristics}

\setlength\extrarowheight{3pt}

\begin{table}[H]
\centering
\caption{Characteristics of dataset}\vspace{1em}
\begin{tabular}{@{}lrr@{}}
\toprule
& \textbf{Subject} & \textbf{Body} \\
\midrule
\# Features before removing stop words & 1074 & 19886 \\
\# Features after removing stop words & 915 & 19386 \\
\# $Class_{nonspam}$ & 600 & 600 \\
\# $Class_{spam}$ & 200 & 200 \\
\bottomrule
\end{tabular}
\label{table:datacharacteristics}
\end{table}

% Show the number of words for each corpus â€“ initially and after removing the stop-words.
The ``bag-of-words" model produced 19886 and 1074 features in the body and subject corpora respectively. After the removal of stop words, there were 19386 unique words remaining in the body corpus, and 915 words in the subject corpus (see \autoref{table:datacharacteristics}).

\section{Feature Selection}

% Does the selection make sense to you given the task? Are the two lists similar?
Feature selection is performed using document frequency. This method involves computing the number of documents a word occurs in, for every word, and selecting the top 200 words with the highest scores to build a classifier.

Shown in \autoref{table:subjcorpus} and \autoref{table:bodycorpus} are the top 100 words for the subject and body corpora respectively and their document frequency score. Removing stop words filters out extremely common words that have little value in classification. It is beneficial in text processing, as it removes low quality features, allowing more significant features to have precedence. Thus, given our task to process natural language text, the selection of words shown makes sense as it gives a better representation of the contents of the emails, and helps improve the accuracy of the classifier. %Classifier has an average accuracy of 93.83\% without removing stop words, and 94.67\% when stop words are removed.

A comparison of \autoref{table:subjcorpus} and \autoref{table:bodycorpus} shows significant disparities in the document frequency of features and word distribution. The frequencies for the subject is significantly lower than that of the body. Whilst some features are shared between subject and body, most features selected are different.

% List the top 100 words for each corpus and show their DF score. Examine the list.
\begin{table}[H]
\caption{Top 100 words in subject corpus and corresponding document frequency (DF) scores}\vspace{1em}
\hskip-.3cm \begin{tabular}{@{}rlrrlrrlr@{}}
\toprule
\textbf{Rank} & \multicolumn{1}{c}{\textbf{Word}} & \textbf{Score}
	& \textbf{Rank} & \multicolumn{1}{c}{\textbf{Word}} & \textbf{Score}
	& \textbf{Rank} & \multicolumn{1}{c}{\textbf{Word}} & \textbf{Score}\\
\midrule
1 & sum & 30 & 35 & business & 6 & 69 & great & 4 \\
2 & summary & 26 & 36 & workshop & 6 & 70 & summer & 4 \\
3 & english & 24 & 37 & speaker & 6 & 71 & home & 3 \\
4 & language & 21 & 38 & german & 6 & 72 & people & 3 \\
5 & free & 20 & 39 & linguist & 5 & 73 & profit & 3 \\
6 & disc & 19 & 40 & list & 5 & 74 & linguists & 3 \\
7 & query & 18 & 41 & native & 5 & 75 & decimal & 3 \\
8 & linguistics & 15 & 42 & resources & 5 & 76 & line & 3 \\
9 & sex & 13 & 43 & grammar & 5 & 77 & credit & 3 \\
10 & comparative & 13 & 44 & time & 5 & 78 & verbal & 3 \\
11 & opposites & 12 & 45 & research & 5 & 79 & video & 3 \\
12 & words & 12 & 46 & word & 5 & 80 & millions & 3 \\
13 & email & 10 & 47 & needed & 5 & 81 & lists & 3 \\
14 & book & 10 & 48 & languages & 5 & 82 & dental & 3 \\
15 & method & 9 & 49 & software & 5 & 83 & life & 3 \\
16 & call & 9 & 50 & jobs & 5 & 84 & resolution & 3 \\
17 & job & 9 & 51 & spanish & 5 & 85 & comparison & 3 \\
18 & japanese & 8 & 52 & fwd & 4 & 86 & offer & 3 \\
19 & correction & 8 & 53 & web & 4 & 87 & latin & 3 \\
20 & announcement & 7 & 54 & phonetics & 4 & 88 & change & 3 \\
21 & syntax & 7 & 55 & american & 4 & 89 & uniformitarianism & 3 \\
22 & million & 7 & 56 & request & 4 & 90 & check & 3 \\
23 & qs & 7 & 57 & banning & 4 & 91 & hey & 3 \\
24 & chinese & 7 & 58 & intuitions & 4 & 92 & reference & 3 \\
25 & program & 7 & 59 & pig & 4 & 93 & debt & 3 \\
26 & lang & 6 & 60 & books & 4 & 94 & synthetic & 3 \\
27 & internet & 6 & 61 & unlimited & 4 & 95 & world & 3 \\
28 & slip & 6 & 62 & programs & 4 & 96 & chomsky & 3 \\
29 & armey & 6 & 63 & systems & 4 & 97 & site & 3 \\
30 & money & 6 & 64 & address & 4 & 98 & released & 3 \\
31 & mail & 6 & 65 & secrets & 4 & 99 & mac & 3 \\
32 & part & 6 & 66 & information & 4 & 100 & sites & 3 \\
33 & conference & 6 & 67 & www & 4 & & & \\
34 & dick & 6 & 68 & read & 4 & & & \\
\bottomrule
\end{tabular}
\label{table:subjcorpus}
\end{table}

\begin{table}[H]
\centering
\caption{Top 100 words in body corpus and corresponding document frequency (DF) scores}\vspace{1em}
\begin{tabular}{@{}rlrrlrrlr@{}}
\toprule
\textbf{Rank} & \multicolumn{1}{c}{\textbf{Word}} & \textbf{Score}
	& \textbf{Rank} & \multicolumn{1}{c}{\textbf{Word}} & \textbf{Score}
	& \textbf{Rank} & \multicolumn{1}{c}{\textbf{Word}} & \textbf{Score}\\
\midrule
1 & information & 205 & 35 & message & 91 & 69 & full & 74 \\
2 & language & 192 & 36 & ll & 89 & 70 & business & 74 \\
3 & mail & 183 & 37 & phone & 88 & 71 & today & 73 \\
4 & university & 179 & 38 & check & 88 & 72 & ac & 73 \\
5 & time & 178 & 39 & receive & 88 & 73 & questions & 72 \\
6 & list & 171 & 40 & good & 87 & 74 & interest & 72 \\
7 & address & 165 & 41 & day & 86 & 75 & remove & 72 \\
8 & english & 159 & 42 & year & 86 & 76 & john & 71 \\
9 & http & 156 & 43 & interested & 86 & 77 & found & 70 \\
10 & linguistics & 156 & 44 & case & 85 & 78 & related & 70 \\
11 & people & 146 & 45 & working & 85 & 79 & site & 69 \\
12 & send & 146 & 46 & include & 85 & 80 & usa & 69 \\
13 & free & 144 & 47 & based & 84 & 81 & linguist & 69 \\
14 & make & 140 & 48 & ve & 84 & 82 & text & 68 \\
15 & email & 133 & 49 & made & 83 & 83 & read & 68 \\
16 & work & 128 & 50 & home & 83 & 84 & point & 68 \\
17 & number & 128 & 51 & note & 83 & 85 & week & 67 \\
18 & www & 122 & 52 & part & 83 & 86 & ago & 67 \\
19 & languages & 119 & 53 & including & 81 & 87 & book & 67 \\
20 & find & 118 & 54 & mailing & 81 & 88 & cost & 66 \\
21 & fax & 116 & 55 & type & 80 & 89 & dear & 66 \\
22 & order & 108 & 56 & place & 79 & 90 & making & 66 \\
23 & call & 103 & 57 & give & 79 & 91 & question & 65 \\
24 & form & 101 & 58 & web & 79 & 92 & simply & 65 \\
25 & research & 100 & 59 & program & 79 & 93 & offer & 63 \\
26 & state & 99 & 60 & special & 78 & 94 & general & 63 \\
27 & linguistic & 99 & 61 & date & 78 & 95 & received & 63 \\
28 & subject & 98 & 62 & line & 78 & 96 & important & 62 \\
29 & years & 98 & 63 & days & 77 & 97 & data & 62 \\
30 & world & 98 & 64 & back & 76 & 98 & ca & 61 \\
31 & contact & 97 & 65 & internet & 76 & 99 & summary & 61 \\
32 & de & 96 & 66 & service & 75 & 100 & long & 61 \\
33 & money & 94 & 67 & american & 75 & & & \\
34 & word & 91 & 68 & system & 74 & & & \\
\bottomrule
\end{tabular}
\label{table:bodycorpus}
\end{table}

\section{Subject vs Body Analysis}

\subsection{Results}

% Results - Fill in the following tables where ZeroR, OneR, NB, DT and MLP are the Wekaâ€™s classifiers tested with Wekaâ€™s 10 fold cross validation; MyNB is your NB tested with your 10-fold cross validation.

\begin{table}[H]
\centering
\caption{Accuracy of various classifiers tested with 10 fold cross validation for the subject and body corpus}\vspace{1em}
\begin{tabular}{@{}lrr@{}}
\toprule
& \multicolumn{2}{c}{\textbf{Accuracy (\%)}} \\
\midrule
\textbf{Classifier} & \textbf{Subject} & \textbf{Body} \\
\midrule
ZeroR & 66.67 & 66.67 \\
OneR & 70.00 & 81.50 \\
1-NN & 75.5 & 87.50 \\
3-NN & 71.17 & 84.33 \\
NB & 69.67 & 95.00 \\
DT & 66.67 & 91.50 \\
MLP & 79.00 & 96.83 \\
MyNB & 66.17 & 94.83 \\
\bottomrule
\end{tabular}
\label{table:results}
\end{table}

\subsection{Discussion}

% Discussion â€“ compare the performance of the classifiers on the 2 corpora, compare your NB with the Wekaâ€™s NB, anything else you consider important.
%
% -correct and deep discussion of the results
% ----comparison between the classifiers (accuracy, training time, other advantages)
% ----myNB vs Wekaâ€™s NB
% ----comparison between the 2 corpora

The overall performance of the classifiers regardless of type, perform significantly better on the body than the subject corpus.

In order to find out if the differences are significant, we can use a paired t-test.

\begin{enumerate}
\item Calculate difference
\end{enumerate}

$$ Z = d_{mean} \pm t_{(1-\alpha)(k-1)} \invbreve\sigma ^{2} $$

% - **T-paired test**
%     1. Calculate the differences _di_
%     2. Calculate the _variance_ of the difference (estimate of the true variance). If k is sufficiently large, _di_ is normally distributed
%     3. Calculate _confidence interval_ _Z_
%     4. If interval contains 0 â€“ difference not significant, else significant
%     - Variance: $\invbreve\sigma ^{2} = \frac{{\sum\nolimits_{j=1}^k (di-d_{mean})}^2 }{k(k-1)}$
%     - Confidence interval: $Z = d_{mean} \pm t_{(1-\alpha)(k-1)} \invbreve\sigma ^{2}$
%         + $1-\alpha$: confidence level; $k-1$ degree of freedom

\section{Challenge Analysis}

% describe what you did and what your results are (accuracy using 10-fold cross validation as above) and discuss the results

\subsection{Results}

\subsection{Discussion}

\section{Conclusion and Future Work}

% summarize your main findings and, if possible, suggest future work.
The tokens used in our classifier are formed from single words. Therefore, it will not analyse common consecutive words that are found in spam emails, leading to the failure of our classifier from detecting these emails.

By taking into account permutations of consecutive words, or words that appear within a specified distance of each other, the accuracy of our Bayesian classifier could be increased.

\section{Reflection}
% what was the most important thing you learned from this assignment? (1-2 paragraphs).

\section{Instructions: How to run code}

\begin{thebibliography}{9}

\bibitem{sebastiani}
  Fabrizio Sebastiani,
  \emph{Machine learning in automated text categorization}.
  ACM Computing Surveys,
  34(1):1-47,
  2002.

\end{thebibliography}

\end{document}