\documentclass[10pt, a4paper]{article}

\usepackage{amssymb,amsmath}
\usepackage{enumitem}

\makeatletter
\def\@seccntformat#1{%
  \expandafter\ifx\csname c@#1\endcsname\c@section\else
  \csname the#1\endcsname\quad
  \fi}
\makeatother

\usepackage{fancyhdr}
\fancyfoot[C]{\thepage}
\lhead{COMP3608}
\rhead{Na\"{i}ve Bayes}
\pagestyle{fancy}

\begin{document}

\section{Email Classification}

\subsection*{Aim}
% State aim of study, and write a paragraph about why the problem is important

The aim of this study is to implement text classification of emails using the Na\"{i}ve Bayes classifier for spam detection.

Spam emails are usually sent to derive cash from the user either directly by tricking the user into purchasing something or indirectly by ticking them into parting with information. The classification of emails aims foremost to protect the unaware user from the malicious intent of spam emails. Classification of emails also saves the user the time and hassle of manually classifying and deleting spam.

\subsection*{Data Preprocessing and Feature Selection}
% Briefly describe what you did.
The files in the sample set \emph{Lingpspam-mini600} are represented as a ``bag-of-words", where words are extracted from the file and treated as a feature. The \emph{document frequency} feature selection method was used, to select the 200 words which occured in the most documents as features, and used to build a classifier for the given documents. The features are then weighted using \emph{td-idf} and normalised using \emph{cosine normalisation}.

\begin{enumerate}
\item For each file in the sample set \emph{Lingpspam-mini600}:
	\begin{enumerate}[label*=\arabic*.]
	\item Open the file and read it line by line.
	\item Replace all punctuation and special symbols from each line with a space. (e.g. ``don't" becomes ``don", ``t")
	\item The sub-corpus the line belongs to is determined by checking if the line begins with ``Subject:". If it does the words within the line will be added to the subject corpus, otherwise they will be added to the body corpus.
	\item Words from each line are extracted using space as a delimiter.
	\item Stop words, numbers and words containing digits are removed.
	\item A counter is created for both corpora. For each word that appears in the email add it to its respective counter.
	\item For each word that appeared in the counter for an email, it is added once to the counter for the corresponding subcorpus.
	\end{enumerate}
\item In each corpus the words with the top 200 Document Frequency (words which are appeared in the most number of documents) were selected as features for that corpus.
\item For each feature, its \emph{tf-idf} score is calculated.
\item The \emph{tf-idf} values are normalised using \emph{cosine normalisation}.
\end{enumerate}

% Show the number of words for each corpus – initially and after removing the stop-words.

The removal of stop words in text processing filters out common words that offer little value for classification. Before stop words were removed, there were 19886 and 1074 unique words in the body and subject corpuses respectively. After their removal, there were 19386 unique words remaining in the body corpus, and 915 words in the subject corpus.

% List the top 100 words for each corpus and show their DF score. Examine the list.
\begin{tabular}{|c|c||c|c||c|c||c|c||c|c|}\centering

Feature & DF & Feature & DF & Feature & DF & Feature & DF & Feature & DF \\
\hline
sum & 30 & qs & 7 & spanish & 5 & secrets & 4 & youthese & 3 \\
summary & 26 & syntax & 7 & list & 5 & address & 4 & offer & 3 \\
english & 24 & chinese & 7 & time & 5 & fwd & 4 & line & 3 \\
language & 21 & million & 7 & word & 5 & read & 4 & home & 3 \\
free & 20 & program & 7 & software & 5 & request & 4 & site & 3 \\
disc & 19 & business & 6 & languages & 5 & information & 4 & check & 3 \\
query & 18 & slip & 6 & linguist & 5 & systems & 4 & reference & 3 \\
linguistics & 15 & conference & 6 & jobs & 5 & american & 4 & background & 3 \\
comparative & 13 & lang & 6 & research & 5 & intuitions & 4 & cd & 3 \\
sex & 13 & part & 6 & native & 5 & great & 4 & teaching & 3 \\
words & 12 & german & 6 & resources & 5 & double & 3 & decimal & 3 \\
opposites & 12 & money & 6 & www & 4 & change & 3 & latin & 3 \\
book & 10 & workshop & 6 & unlimited & 4 & synthetic & 3 & names & 3 \\
email & 10 & armey & 6 & programs & 4 & credit & 3 & counting & 3 \\
method & 9 & speaker & 6 & web & 4 & requested & 3 & ipa & 3 \\
job & 9 & dick & 6 & banning & 4 & future & 3 & corpus & 3 \\
call & 9 & mail & 6 & phonetics & 4 & tonight & 3 & complete & 3 \\
japanese & 8 & internet & 6 & summer & 4 & make & 3 & world & 3 \\
correction & 8 & grammar & 5 & books & 4 & comparison & 3 & resolution & 3 \\
announcement & 7 & needed & 5 & pig & 4 & hey & 3 & dialect & 3 \\
\hline

\end{tabular}


\begin{tabular}{|c|c||c|c||c|c||c|c||c|c|}\centering

Feature & DF & Feature & DF & Feature & DF & Feature & DF & Feature & DF \\
\hline
information & 205 & fax & 116 & interested & 86 & special & 78 & site & 69 \\
language & 192 & order & 108 & year & 86 & line & 78 & text & 68 \\
mail & 183 & call & 103 & day & 86 & days & 77 & read & 68 \\
university & 179 & form & 101 & working & 85 & internet & 76 & point & 68 \\
time & 178 & research & 100 & include & 85 & back & 76 & week & 67 \\
list & 171 & linguistic & 99 & case & 85 & american & 75 & ago & 67 \\
address & 165 & state & 99 & based & 84 & service & 75 & book & 67 \\
english & 159 & subject & 98 & ve & 84 & system & 74 & dear & 66 \\
linguistics & 156 & years & 98 & note & 83 & business & 74 & cost & 66 \\
http & 156 & world & 98 & home & 83 & full & 74 & making & 66 \\
people & 146 & contact & 97 & made & 83 & ac & 73 & question & 65 \\
send & 146 & de & 96 & part & 83 & today & 73 & simply & 65 \\
free & 144 & money & 94 & including & 81 & interest & 72 & offer & 63 \\
make & 140 & message & 91 & mailing & 81 & questions & 72 & received & 63 \\
email & 133 & word & 91 & type & 80 & remove & 72 & general & 63 \\
number & 128 & ll & 89 & web & 79 & john & 71 & data & 62 \\
work & 128 & check & 88 & give & 79 & related & 70 & important & 62 \\
www & 122 & phone & 88 & program & 79 & found & 70 & ca & 61 \\
languages & 119 & receive & 88 & place & 79 & linguist & 69 & summary & 61 \\
find & 118 & good & 87 & date & 78 & usa & 69 & long & 61 \\
\hline

\end{tabular}


% Does the selection make sense to you given the task? Are the two lists similar?

\subsection*{Subject vs Body: Results and Discussion}
% Results - Fill in the following tables where ZeroR, OneR, NB, DT and MLP are the Weka’s classifiers tested with Weka’s 10 fold cross validation; MyNB is your NB tested with your 10-fold cross validation.

\begin{tabular}{|l|l|}
\hline 
\multicolumn{2}{|l|}{Corpus: Subject} \\ 
\hline 
• & Accuracy [\%] \\ 
\hline 
ZeroR & • \\ 
\hline 
OneR & • \\ 
\hline 
1-NN & • \\ 
\hline 
3-NN & • \\ 
\hline 
NB & • \\ 
\hline 
DT & • \\ 
\hline 
MLP & • \\ 
\hline 
MyNB & • \\ 
\hline 
\end{tabular} 

\begin{tabular}{|l|l|}
\hline 
\multicolumn{2}{|l|}{Corpus: Body} \\ 
\hline 
• & Accuracy [\%] \\ 
\hline 
ZeroR & • \\ 
\hline 
OneR & • \\ 
\hline 
1-NN & • \\ 
\hline 
3-NN & • \\ 
\hline 
NB & • \\ 
\hline 
DT & • \\ 
\hline 
MLP & • \\ 
\hline 
MyNB & • \\ 
\hline 
\end{tabular} 

% Discussion – compare the performance of the classifiers on the 2 corpora, compare your NB with the Weka’s NB, anything else you consider important.

\subsection*{Challenge Results and Discussion}
% describe what you did and what your results are (accuracy using 10-fold cross validation as above) and discuss the results

\subsection*{Conclusions}
% summarize your main findings and, if possible, suggest future work.

The tokens used in our classifier are formed from single words. Therefore, it will not analyse common consecutive words that are found in spam emails, leading to the failure of our classifier from detecting these emails. 

By taking into account permutations of consecutive words, or words that appear within a specified distance of each other, the accuracy of our Bayesian classifier could be increased. 

\subsection*{Reflection}
% what was the most important thing you learned from this assignment? (1-2 paragraphs).

\subsection*{Instructions: How to run code}

\end{document}